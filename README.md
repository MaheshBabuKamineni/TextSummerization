# Text Summarization using NLP for Machine Learning Wikipedia Page

This project aims to provide an implementation of text summarization techniques using Natural Language Processing (NLP) to summarize the Wikipedia page on Machine Learning. The goal is to extract the most important information and present it in a concise and readable format.

## Table of Contents
- [Overview](#overview)
- [Features](#features)
- [Technologies](#technologies)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Results](#results)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)

## Overview
Text summarization is a crucial task in NLP that involves creating a short, accurate, and fluent summary of a longer text document. This project focuses on both extractive and abstractive summarization techniques to condense the content of the Machine Learning Wikipedia page.

## Features
- Extractive summarization using frequency-based methods and TextRank.
- Abstractive summarization using transformer models like BERT and GPT-3.
- Preprocessing techniques including tokenization, stop word removal, and stemming.
- Evaluation metrics for summarization performance.

## Technologies
- Python
- NLTK
- Gensim
- Hugging Face Transformers
- SpaCy
- Heapfy

## Installation
To get started with this project, follow these steps:

1. Clone the repository:
    ```bash
    git clone https://github.com/yourusername/TextSummarization.git
    cd TextSummarization
    ```

2. Create a virtual environment:
    ```bash
    py
